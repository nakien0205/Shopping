{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75272462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "train = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
    "test = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")\n",
    "\n",
    "client = chromadb.Client()\n",
    "# Create a table to store data\n",
    "collection = client.get_or_create_collection('Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46b569ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['test']['answer'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f70e25",
   "metadata": {},
   "source": [
    "## Test info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3570a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "vector_db = pd.read_csv('embed.csv')\n",
    "\n",
    "embed_db = []\n",
    "text_db = []\n",
    "\n",
    "vector_db['1'] = vector_db['1'].apply(ast.literal_eval)  # Convert string representation of list to actual list\n",
    "for i in range(3200):\n",
    "    chunk = vector_db['0'][i]\n",
    "    embed = vector_db['1'][i]\n",
    "\n",
    "    text_db.append(chunk)\n",
    "    embed_db.append(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515b3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    ids=[str(ids) for ids in range(3200)],\n",
    "    embeddings=embed_db,\n",
    "    metadatas=[{\"source\": f'Document: {i}', 'text': text_db[i]} for i in range(3200)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d59c6a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['697', '279']],\n",
       " 'embeddings': None,\n",
       " 'documents': [[None, None]],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'source': 'Document: 697',\n",
       "    'text': 'Sixteen months before his death, his son, John Quincy Adams, became the sixth President of the United States (1825 1829), the only son of a former President to hold the office until George W. Bush in 2001.'},\n",
       "   {'source': 'Document: 279',\n",
       "    'text': 'Lincoln closely supervised the victorious war effort, especially the selection of top generals, including Ulysses S. Grant. Historians have concluded that he handled the factions of the Republican Party well, bringing leaders of each faction into his cabinet and forcing them to cooperate. Lincoln successfully defused a war scare with the United Kingdom in 1861. Under his leadership, the Union took control of the border slave states at the start of the war. Additionally, he managed his own reelection in the 1864 presidential election.'}]],\n",
       " 'distances': [[0.5614844560623169, 0.5666584968566895]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "\n",
    "llama_embeder = OllamaEmbeddingFunction(model_name='llama3.2:1B')\n",
    "res_list = []\n",
    "test_num = 100\n",
    "\n",
    "for i in range(test_num):\n",
    "    input_query = test['test']['question'][i] \n",
    "\n",
    "    q_embed = llama_embeder([input_query])\n",
    "\n",
    "    res_list.append(collection.query(\n",
    "        query_embeddings=q_embed,\n",
    "        n_results=2\n",
    "    ))\n",
    "\n",
    "res_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91a8255d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4386f3d62624f6cac924451a020b0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "from tqdm._tqdm_notebook import tqdm\n",
    "\n",
    "eval_list = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "  instruction_prompt = f'''\n",
    "  You are a helpful chatbot that gives a concise and short answer.\n",
    "  Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "  {res_list[i]['metadatas'][0][0]['text'], res_list[i]['metadatas'][0][1]['text']}\n",
    "  '''\n",
    "\n",
    "  eval_prompt = '''\n",
    "  You are an answer evaluator. Compare the correct answer with the predicted answer and determine if they match.\n",
    "  The predicted answer must be CONSISTENT and CLEAR throughout - contradictory statements should be marked as incorrect.\n",
    "\n",
    "  You can ONLY output:\n",
    "  - \"1\" if the predicted answer is correct, consistent, and substantially matches the correct answer\n",
    "  - \"0\" if the predicted answer is incorrect, contradictory, unclear, or does not match the correct answer\n",
    "\n",
    "  Do not provide any explanation, just output the number.\n",
    "  '''\n",
    "\n",
    "  answer = ollama.chat(\n",
    "      model='llama3.2:1B',\n",
    "      messages=[\n",
    "        {'role': 'system', 'content': instruction_prompt},\n",
    "        {'role': 'user', 'content': test['test']['question'][i]},\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  # print the response from the chatbot in real-time\n",
    "  # print('Chatbot response:')\n",
    "  # print(answer['message']['content'])\n",
    "\n",
    "  # Get the correct answer from your test dataset\n",
    "  correct_answer = test['test']['answer'][i]  # Adjust index as needed\n",
    "  predicted_answer = answer['message']['content']\n",
    "\n",
    "  evaluation_input = f\"Correct answer: {correct_answer}\\nPredicted answer: {predicted_answer}\"\n",
    "\n",
    "  evaluation = ollama.chat(\n",
    "      model='llama3.2:1B',\n",
    "      messages=[\n",
    "        {'role': 'system', 'content': eval_prompt},\n",
    "        {'role': 'user', 'content': evaluation_input},\n",
    "      ]\n",
    "  )\n",
    "\n",
    "\n",
    "  evaluation_score = evaluation['message']['content'].strip()\n",
    "  eval_list.append(int(evaluation_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4560788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in eval_list:\n",
    "    if i == 1:\n",
    "        correct += 1\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a15329da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18 months'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['test']['answer'][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
